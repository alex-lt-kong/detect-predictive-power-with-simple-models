# handwriting-recognition

This project is NOT meant to reproduce how well a neural network (no, CNN is not involved...) or other modern machine learning models can do the job. Instead, it tries to understand one question: say we know that feature set **X** does have very good predictive power but the cause and effect relationship is highly non-linear, is it possible to use a very simple model to discover it without relying on domain knowledge?

If the answer to this question is yes, then it implies that, while having a fantastic (i.e., very accurate) model is hard, it is comparatively easy to decide whether or not some features have predictive power (since very simple models can detect its existence). In this particular case, say a fine-tuned neural network can be 99% accurate in recognizing hand-written text, a linear model which is 55%-60% accurate could lead to a very strong conclusion that causality does exist (just a linear model is not flexible enough to uncover it).

The significance behind this question is that, usually artificial intelligence performs pretty well in some areas traditional programs fail to work, such as facial recognition or natural language processing. While these achievements are definitely amazing, they are all the same in one aspect--we already know there exists strong causality--since we human beings can usually do it with ease. However, sometimes we try to predict the **y** without knowing if the **X** we have has any predictive power at all. In this case, suppose our models are totally useless (i.e., doing not better than random guess), will simply digging deeper (e.g., adding more leaves or more layers of neurons) work? Or does it mean that it is most likely the dataset we have just does not have the predictive power so digging deeper usually produces nothing at best and a high level of overfitting at worst?